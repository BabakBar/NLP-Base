{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words and sentiment analysis\n",
    "# count vectorizer, transforming the vectorizer\n",
    "# A bag-of-words is an approach to transform text to numeric form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 1 0 1 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "# Import the required function\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "annak = ['Happy families are all alike;', 'every unhappy family is unhappy in its own way']\n",
    "\n",
    "# Build the vectorizer and fit it\n",
    "anna_vect = CountVectorizer()\n",
    "anna_vect.fit(annak)\n",
    "\n",
    "# Create the bow representation\n",
    "anna_bow = anna_vect.transform(annak)\n",
    "\n",
    "# Print the bag-of-words result \n",
    "print(anna_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>score</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite musi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Excellent Soundtrack: I truly like this sound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After H...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  score                                             review\n",
       "0           0      1   Stuning even for the non-gamer: This sound tr...\n",
       "1           1      1   The best soundtrack ever to anything.: I'm re...\n",
       "2           2      1   Amazing!: This soundtrack is my favorite musi...\n",
       "3           3      1   Excellent Soundtrack: I truly like this sound...\n",
       "4           4      1   Remember, Pull Your Jaw Off The Floor After H..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOW using product reviews\n",
    "reviews = pd.read_csv(\"Data/amazon_reviews_sample.csv\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   about  after  all  also  am  an  and  any  are  as  ...  what  when  which  \\\n",
      "0      0      0    1     0   0   0    2    0    0   0  ...     0     0      0   \n",
      "1      0      0    0     0   0   0    3    1    1   0  ...     0     0      0   \n",
      "2      0      0    3     0   0   1    4    0    1   1  ...     0     0      1   \n",
      "3      0      0    0     0   0   0    9    0    1   0  ...     0     0      0   \n",
      "4      0      1    0     0   0   0    3    0    1   0  ...     0     0      0   \n",
      "\n",
      "   who  will  with  work  would  you  your  \n",
      "0    2     0     1     0      2    0     1  \n",
      "1    0     0     0     0      1    1     0  \n",
      "2    1     0     0     1      1    2     0  \n",
      "3    0     0     0     0      0    0     0  \n",
      "4    0     0     0     0      0    3     1  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# Build the vectorizer, specify max features \n",
    "vect = CountVectorizer(max_features=100)\n",
    "# Fit the vectorizer\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create the bow representation\n",
    "X_df=pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-grams\n",
    "# There is a way to capture the context when using a BOW by, for example, considering pairs or triples of tokens that appear next to each other. Let's define some terms. Single tokens are what we used so far and are also called 'unigrams'. Bigrams are pairs of tokens, trigrams are triples of tokens, and a sequence of n-tokens is called 'n-grams.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  00 and  00 apiece  00 as  00 back  00 bad  00 bucks  00 cheaper  00 do  \\\n",
      "0   0       0          0      0        0       0         0           0      0   \n",
      "1   0       0          0      0        0       0         0           0      0   \n",
      "2   0       0          0      0        0       0         0           0      0   \n",
      "3   0       0          0      0        0       0         0           0      0   \n",
      "4   0       0          0      0        0       0         0           0      0   \n",
      "\n",
      "   00 does  ...  étai fidèle  était  était pas  étre  étre publié  éviter  \\\n",
      "0        0  ...            0      0          0     0            0       0   \n",
      "1        0  ...            0      0          0     0            0       0   \n",
      "2        0  ...            0      0          0     0            0       0   \n",
      "3        0  ...            0      0          0     0            0       0   \n",
      "4        0  ...            0      0          0     0            0       0   \n",
      "\n",
      "   última  última parte  única  única opción  \n",
      "0       0             0      0             0  \n",
      "1       0             0      0             0  \n",
      "2       0             0      0             0  \n",
      "3       0             0      0             0  \n",
      "4       0             0      0             0  \n",
      "\n",
      "[5 rows x 326726 columns]\n"
     ]
    }
   ],
   "source": [
    "# Build the vectorizer, specify token sequence and fit\n",
    "# uni- and bigrams (1,2)\n",
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of vocabulary of movies reviews\n",
    "# The three methods that you will use will transform the text column to new numeric columns, capturing the count of a word or a phrase in each review. Each method will ultimately result in building a different number of new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   about  after  all  also  am  an  and  any  are  as  ...  what  when  which  \\\n",
      "0      0      0    1     0   0   0    2    0    0   0  ...     0     0      0   \n",
      "1      0      0    0     0   0   0    3    1    1   0  ...     0     0      0   \n",
      "2      0      0    3     0   0   1    4    0    1   1  ...     0     0      1   \n",
      "3      0      0    0     0   0   0    9    0    1   0  ...     0     0      0   \n",
      "4      0      1    0     0   0   0    3    0    1   0  ...     0     0      0   \n",
      "\n",
      "   who  will  with  work  would  you  your  \n",
      "0    2     0     1     0      2    0     1  \n",
      "1    0     0     0     0      1    1     0  \n",
      "2    1     0     0     1      1    2     0  \n",
      "3    0     0     0     0      0    0     0  \n",
      "4    0     0     0     0      0    3     1  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# Build the vectorizer, specify size of vocabulary and fit\n",
    "# max_features - max_df - min_df\n",
    "vect = CountVectorizer(max_features=100)\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   able to  about how  about it  about the  about this  after reading  \\\n",
      "0        0          0         0          0           0              0   \n",
      "1        0          0         0          0           0              0   \n",
      "2        0          0         0          0           0              0   \n",
      "3        0          0         0          0           0              0   \n",
      "4        0          0         0          0           0              0   \n",
      "\n",
      "   after the  again and  ago and  agree with  ...  you think  you to  you ve  \\\n",
      "0          0          0        0           0  ...          0       0       0   \n",
      "1          0          0        0           0  ...          0       0       0   \n",
      "2          0          0        0           0  ...          0       0       2   \n",
      "3          0          0        0           0  ...          0       0       0   \n",
      "4          0          0        0           0  ...          0       0       1   \n",
      "\n",
      "   you want  you will  you won  you would  your money  your own  your time  \n",
      "0         0         0        0          0           0         0          0  \n",
      "1         0         0        0          0           0         0          0  \n",
      "2         0         0        0          0           0         0          0  \n",
      "3         0         0        0          0           0         0          0  \n",
      "4         0         0        0          0           0         0          0  \n",
      "\n",
      "[5 rows x 1000 columns]\n"
     ]
    }
   ],
   "source": [
    "# BOW with n-grams and vocabulary size\n",
    "# Build the vectorizer and make sure to specify the following parameters: the size of the vocabulary should be limited to 1000, include only bigrams, and ignore terms that appear in more than 500 documents.\n",
    "\n",
    "# Build the vectorizer, specify max features and fit\n",
    "vect = CountVectorizer (max_features=1000, ngram_range=(2, 2), max_df=500)\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create a DataFrame from the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build new features from text\n",
    "We could measure how long each review is, how many sentences it contains, or say something about the parts of speech involved, punctuation marks, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Never', 'forget', 'what', 'you', 'are', ',', 'for', 'surely', 'the', 'world', 'will', 'not', '.', 'Make', 'it', 'your', 'strength', '.', 'Then', 'it', 'can', 'never', 'be', 'your', 'weakness', '.', 'Armour', 'yourself', 'in', 'it', ',', 'and', 'it', 'will', 'never', 'be', 'used', 'to', 'hurt', 'you', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a string from GoT\n",
    "\n",
    "GoT = 'Never forget what you are, for surely the world will not. Make it your strength. Then it can never be your weakness. Armour yourself in it, and it will never be used to hurt you.'\n",
    "\n",
    "# Import the required function\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Transform the GoT string to word tokens\n",
    "print(word_tokenize(GoT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Cause', 'if', 'we', 'ca', \"n't\", 'protect', 'the', 'Earth', ',', 'you', 'can', 'be', 'd', '*', '*', '*', 'sure', 'we', \"'ll\", 'avenge', 'it'], ['There', 'was', 'an', 'idea', 'to', 'bring', 'together', 'a', 'group', 'of', 'remarkable', 'people', ',', 'to', 'see', 'if', 'we', 'could', 'become', 'something', 'more'], ['These', 'guys', 'come', 'from', 'legend', ',', 'Captain', '.', 'They', \"'re\", 'basically', 'Gods', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Word tokens from the Avengers\n",
    "avengers = [\"Cause if we can't protect the Earth, you can be d*** sure we'll avenge it\",\n",
    " 'There was an idea to bring together a group of remarkable people, to see if we could become something more',\n",
    " \"These guys come from legend, Captain. They're basically Gods.\"]\n",
    "\n",
    "# Tokenize each item in the avengers \n",
    "tokens_avengers = [word_tokenize(item) for item in avengers]\n",
    "\n",
    "print(tokens_avengers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stuning', 'even', 'for', 'the', 'non-gamer', ':', 'This', 'sound', 'track', 'was', 'beautiful', '!', 'It', 'paints', 'the', 'senery', 'in', 'your', 'mind', 'so', 'well', 'I', 'would', 'recomend', 'it', 'even', 'to', 'people', 'who', 'hate', 'vid', '.', 'game', 'music', '!', 'I', 'have', 'played', 'the', 'game', 'Chrono', 'Cross', 'but', 'out', 'of', 'all', 'of', 'the', 'games', 'I', 'have', 'ever', 'played', 'it', 'has', 'the', 'best', 'music', '!', 'It', 'backs', 'away', 'from', 'crude', 'keyboarding', 'and', 'takes', 'a', 'fresher', 'step', 'with', 'grate', 'guitars', 'and', 'soulful', 'orchestras', '.', 'It', 'would', 'impress', 'anyone', 'who', 'cares', 'to', 'listen', '!', '^_^']\n"
     ]
    }
   ],
   "source": [
    "# A feature for the length of a review\n",
    "\n",
    "# Tokenize each item in the review column \n",
    "word_tokens = [word_tokenize(review) for review in reviews.review]\n",
    "\n",
    "# Print out the first item of the word_tokens list\n",
    "print(word_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>score</th>\n",
       "      <th>review</th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tr...</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm re...</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite musi...</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Excellent Soundtrack: I truly like this sound...</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After H...</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  score                                             review  \\\n",
       "0           0      1   Stuning even for the non-gamer: This sound tr...   \n",
       "1           1      1   The best soundtrack ever to anything.: I'm re...   \n",
       "2           2      1   Amazing!: This soundtrack is my favorite musi...   \n",
       "3           3      1   Excellent Soundtrack: I truly like this sound...   \n",
       "4           4      1   Remember, Pull Your Jaw Off The Floor After H...   \n",
       "\n",
       "   n_words  \n",
       "0       87  \n",
       "1      109  \n",
       "2      165  \n",
       "3      145  \n",
       "4      109  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty list to store the length of reviews\n",
    "len_tokens = []\n",
    "\n",
    "# Iterate over the word_tokens list and determine the length of each item\n",
    "for i in range(len(word_tokens)):\n",
    "    len_tokens.append(len(word_tokens[i]))\n",
    "# Create a new feature for the lengh of each review\n",
    "reviews['n_words'] = len_tokens \n",
    "reviews.head()\n",
    "# You have used a list comprehension and a for loop to iterate over the word tokens created from the review column. You can employ the same approach to create other features, such as one counting the number of sentences in each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot use a string pattern on a bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d27f64bc683e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Loop over the rows of the dataset and append\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mlanguages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect_langs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Clean the list by splitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langdetect/detector_factory.py\u001b[0m in \u001b[0;36mdetect_langs\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0minit_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_factory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langdetect/detector.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mDetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_max_text_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrest\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcut\u001b[0m \u001b[0mdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         '''\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURL_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAIL_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNGram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_vi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot use a string pattern on a bytes-like object"
     ]
    }
   ],
   "source": [
    "# detect the language\n",
    "# analyze the sentiment of non-English text. first task in such a case will be to identify the foreign language.\n",
    "from langdetect import detect_langs\n",
    "\n",
    "languages = [] \n",
    "\n",
    "# Loop over the rows of the dataset and append  \n",
    "for row in range(len(reviews)):\n",
    "    languages.append(detect_langs(reviews.iloc[row, 1]))\n",
    "\n",
    "# Clean the list by splitting     \n",
    "languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
    "\n",
    "# Assign the list to a new feature \n",
    "reviews['language'] = languages\n",
    "\n",
    "print(reviews.head())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90d056296a45556521e929369b7ac4560abf6ad78fcd7dc94e39c36823bee644"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
